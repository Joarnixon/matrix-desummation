{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is desummation?\n",
    "\n",
    "Well, by that I mean representing a given matrix A as a sum of matrices $\\cdot B_i$ with corresponding weights or coefficients $\\cdot w_i$\n",
    "\n",
    "$A = w_1 \\cdot B_1 + w_2 \\cdot B_2 + \\ldots + w_n \\cdot B_n$\n",
    "\n",
    "I suppose that the reader is familiar with basics of linear algebra and can remember used facts with no difficulties.\n",
    "\n",
    "### Some popular mathematical theorems on this topic:\n",
    "\n",
    " - Spectral decomposition of a symmetric matrix:\n",
    "  $A = Q \\Lambda Q^T = \\sum_{i=1}^{n} \\lambda_i*u_iu_i^T = \\sum_{i=1}^{n} \\lambda_i * U_i $\n",
    "\n",
    "#### Actually this theorem works for any matrix but with some modification:\n",
    "- Spectral decomposition for diagonalizable matrix:\n",
    "  $ A = S \\Lambda S^{-1} = \\sum_{i=1}^{n} S  \\begin{pmatrix}\n",
    "    0 & 0 & 0 & \\ldots & 0 \\\\\n",
    "    0 & \\ddots & 0 & \\ldots & 0 \\\\\n",
    "    0 & 0 & \\lambda_i & \\ldots & 0 \\\\\n",
    "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    0 & 0 & 0 & \\ldots & 0 \\\\\n",
    "\\end{pmatrix} $ $S^{-1}= \\sum_{i=1}^{n}M_i$\n",
    "\n",
    "#### Or one may use Singular Value Decomposition with the same approach\n",
    "\n",
    "###### I randomly encountered kinda same theorem for tensors, but for now I will stick with simpler perspective.\n",
    "\n",
    "### One thing, that unites these methods is that the matrices are lower in rank than original\n",
    " - This is one of the methods of feature space dimensionality reduction in ML, but we actually don't decompose it to sum. \n",
    " \n",
    " - Instead we take smallest singular values and replace with zero obtaining lower rank approximation that (can be proven) will be the best amongst all other matrices that rank (by Frobenius norm).\n",
    "\n",
    "### Motivation:\n",
    "- Very naive and simple: experiment with matrix topology and make some research.\n",
    "- Maybe some matrices in given problem `can approximate` any matrices `better` ` than the others?\n",
    "- What if we use this for `pictures`? We might lose some information in process, but instead we gain $\\ldots$ weights(*)\n",
    "- What if these weights(*) are some sort of `coordinate space` (of course with fixed basis of $B_i$ matrices)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Now we are ready to start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from desummation import Desummation\n",
    "dsm = Desummation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
